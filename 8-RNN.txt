import numpy as np
# Initialize Parameters
input_size = 2
hidden_size = 3
output_size = 1
Wx = np.random.rand(hidden_size, input_size) * 0.01
Wh = np.random.rand(hidden_size, hidden_size) * 0.01
Wy = np.random.rand(output_size, hidden_size) * 0.01
bh = np.zeros((hidden_size, 1))
by = np.zeros((output_size, 1))
# Define Activation Function
def tanh(x):
    return np.tanh(x)
def tanh_derivative(x):
    return 1 - np.tanh(x) ** 2
 # Forward Pass
def rnn_forward(inputs):
    h = np.zeros((hidden_size, 1))
    for x in inputs:
        x = x.reshape(-1, 1)
        h = tanh(np.dot(Wx, x) + np.dot(Wh, h) + bh)
    y = np.dot(Wy, h) + by
    return y, h
# Backward Pass
def rnn_backward(inputs, target, y, h):
    loss = (y - target) ** 2
    dWy = (y - target) * h.T
    dby = (y - target)
    dh = np.dot(Wy.T, (y - target))
    
    dWx_total = np.zeros_like(Wx)
    dWh_total = np.zeros_like(Wh)
    dbh_total = np.zeros_like(bh)
    for t in reversed(range(len(inputs))):
        x = inputs[t].reshape(-1, 1)
        dhraw = dh * tanh_derivative(h)
        dWx = np.dot(dhraw, x.T)
        dWh = np.dot(dhraw, h.T)
        dbh = dhraw
        dh = np.dot(Wh.T, dhraw)
        
        dWx_total += dWx
        dWh_total += dWh
        dbh_total += dbh
    return dWx_total, dWh_total, dWy, dbh_total, dby

# Update Weights
def update_parameters(learning_rate, dWx, dWh, dWy, dbh, dby):
    global Wx, Wh, Wy, bh, by
    Wx -= learning_rate * dWx
    Wh -= learning_rate * dWh
    Wy -= learning_rate * dWy
    bh -= learning_rate * dbh
    by -= learning_rate * dby
inputs = [np.array([1, 0]), np.array([0, 1]), np.array([1, 1])]
target = np.array([[1]])
learning_rate = 0.01

for epoch in range(100):
    y, h = rnn_forward(inputs)
    dWx, dWh, dWy, dbh, dby = rnn_backward(inputs, target, y, h)     
    update_parameters(learning_rate, dWx, dWh, dWy, dbh, dby)
    if epoch % 10 == 0:
       print(f'Epoch {epoch}, Loss: {np.mean((y - target) ** 2)}')